{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476d0a19-63a3-4fd2-b88d-238bd2cd4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeautoencoder as tae\n",
    "import timediffusion as tdf\n",
    "import DP_Sliding as dp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time \n",
    "import process_edited as pce\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "import timeautoencoder_impute as taei\n",
    "import CSDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043dad3-1f7a-477a-a7aa-892eed640766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETTh-small : https://github.com/zhouhaoyi/ETDataset/tree/main\n",
    "time_duration = []\n",
    "\n",
    "data = 'C:/Users/namjo/OneDrive/Desktop/TimeAutoDiff/Dataset/stock_data'\n",
    "filename = f'{data}.csv'\n",
    "\n",
    "### Read dataframe\n",
    "print(filename)\n",
    "real_df = pd.read_csv(filename)\n",
    "real_df1 = real_df.drop('date', axis=1).iloc[:2000,:]\n",
    "real_df2 = real_df.iloc[:2000,:]\n",
    "\n",
    "##############################################################################################################################\n",
    "### Pre-processing Data\n",
    "threshold = 1; device = 'cuda'\n",
    "processed_data = dp.splitData(real_df1, 48, 1).to(device)\n",
    "time_info = dp.splitTimeData(real_df2, 48).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c660f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter settings : See Appendix D in the paper\n",
    "##############################################################################################################################\n",
    "# Auto-encoder Training\n",
    "n_epochs = 20000; eps = 1e-5\n",
    "weight_decay = 1e-6 ; lr = 2e-4; hidden_dim = 200; num_layers = 2; batch_size = 100\n",
    "channels = 64; min_beta = 1e-5; max_beta = 0.1; emb_dim = 128; time_dim = 8; lat_dim = processed_data.shape[2]; threshold = 1\n",
    "ds = tae.train_autoencoder(real_df1, processed_data, channels, hidden_dim, num_layers, lr, weight_decay, n_epochs, \\\n",
    "                           batch_size, threshold,  min_beta, max_beta, emb_dim, time_dim, lat_dim, device)\n",
    "\n",
    "##############################################################################################################################\n",
    "# Diffusion Training\n",
    "latent_features = ds[1]; \n",
    "n_epochs = 20000; hidden_dim = 200; num_layers = 2; diffusion_steps = 100; lambd = 1;\n",
    "diff = tdf.train_diffusion(latent_features, time_info.to(device), hidden_dim, num_layers, diffusion_steps, lambd, n_epochs)\n",
    "\n",
    "##############################################################################################################################\n",
    "# Sampling new data\n",
    "N, T, F = processed_data.shape\n",
    "t_grid = torch.linspace(0, 1, T).view(1, -1, 1).to(device) # Note that we can use different sequence length here without any issues\n",
    "samples = tdf.sample(t_grid.repeat(N, 1, 1), N, T, lat_dim, diff, time_info.to(device))\n",
    "\n",
    "# Post-process the generated data \n",
    "gen_output = ds[0].decoder(samples[0].to(device))  # Apply decoder to generated latent vector\n",
    "\n",
    "data_size, seq_len, _ = processed_data.shape\n",
    "synth_data = pce.convert_to_tensor(real_df1, gen_output, threshold, N, T)\n",
    "_synth_data = pce.convert_to_table(real_df1, synth_data, threshold)\n",
    "_real_data = pce.convert_to_table(real_df1, processed_data, threshold)\n",
    "\n",
    "B, L, K = _synth_data.shape\n",
    "\n",
    "pd_reshaped = _real_data.reshape(B * L, K)\n",
    "sd_reshaped = _synth_data.cpu().reshape(B * L, K)\n",
    "\n",
    "real_df = pd.DataFrame(pd_reshaped.cpu().numpy())\n",
    "synth_df = pd.DataFrame(sd_reshaped.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e786b90-b3d6-45ea-ba39-9bad6d59d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = pce.DataFrameParser().fit(real_df1, threshold)\n",
    "col_name = parser.column_name()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=K, figsize=(33.1, 23.4/5))\n",
    "\n",
    "for k in range(K):\n",
    "    axes[k].hist(pd_reshaped[:, k].cpu().detach(), bins=50, color='blue', alpha=0.5, label='Real')\n",
    "    axes[k].hist(sd_reshaped[:, k].cpu().detach(), bins=50, color='red', alpha=0.5, label='Synthetic')\n",
    "\n",
    "    # Adding legends\n",
    "    axes[k].legend()\n",
    "    axes[k].set_title(col_name[k], fontsize=15)\n",
    "    \n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)  # Add space between histograms\n",
    "plt.savefig('hurricane.png', dpi=500)  # Adjust dpi as needed for quality\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b929b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeautoencoder as tae\n",
    "import timediffusion as tdf\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import process_edited as pce\n",
    "import correl as correl\n",
    "import Metrics as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import predictive_metrics as pdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71121f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Evaluate Metrics #################### \n",
    "iterations = 2000\n",
    "result_disc = []; result_pred = []; result_tmp = []; result_corr = []\n",
    "\n",
    "for i in range(1):\n",
    "    random_integers = [random.randint(0, len(real_df)-1) for _ in range(2000)]\n",
    "    \n",
    "    a = mt.discriminative_score_metrics(_real_data, _synth_data, iterations)\n",
    "    b = pdm.predictive_score_metrics(_real_data, _synth_data, 7)\n",
    "    c = mt.temp_disc_score(_real_data, _synth_data, iterations)\n",
    "    d = correl.final_correlation(real_df.iloc[random_integers,:], synth_df.iloc[random_integers,:])\n",
    "    \n",
    "    result_disc.append(a)\n",
    "    result_pred.append(b)\n",
    "    result_tmp.append(c)\n",
    "    result_corr.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(result_disc),np.std(result_disc))\n",
    "print(np.mean(result_pred),np.std(result_pred))\n",
    "print(np.mean(result_tmp),np.std(result_tmp))\n",
    "print(np.mean(result_corr),np.std(result_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6300d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_synth_data\n",
    "torch.save(_synth_data, 'stock_data_for_volatility_synthetic.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e216056-00f4-4f6d-8b85-040241f9f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume _real_data and _synth_data are your tensors with size [1953, 48, 8]\n",
    "\n",
    "# Choose an arbitrary sample from _synth_data\n",
    "random_index = torch.randint(0, _synth_data.shape[0], (1,)).item()\n",
    "\n",
    "# The selected sample from _synth_data\n",
    "chosen_synth_data = _synth_data[random_index]\n",
    "\n",
    "# Choose two arbitrary features (randomly select two indices from the third dimension)\n",
    "feature_indices = torch.randperm(_synth_data.shape[2])[:2]\n",
    "feature_1_idx, feature_2_idx = feature_indices.tolist()\n",
    "\n",
    "# Compute Frobenius distance between the chosen _synth_data and every single _real_data\n",
    "frobenius_distances = torch.norm(_real_data - chosen_synth_data, dim=[1, 2])\n",
    "\n",
    "# Find the real data that gives us the closest point to the chosen _synth_data\n",
    "closest_real_data_index = torch.argmin(frobenius_distances).item()\n",
    "closest_real_data = _real_data[closest_real_data_index]\n",
    "\n",
    "# Select the two arbitrary features from the closest _real_data and _synth_data\n",
    "real_feature_1 = closest_real_data[:, feature_1_idx]  # First feature from _real_data\n",
    "real_feature_2 = closest_real_data[:, feature_2_idx]  # Second feature from _real_data\n",
    "\n",
    "synth_feature_1 = chosen_synth_data[:, feature_1_idx]  # First feature from _synth_data\n",
    "synth_feature_2 = chosen_synth_data[:, feature_2_idx]  # Second feature from _synth_data\n",
    "\n",
    "# Function to calculate volatility and moving averages\n",
    "def calculate_metrics(values):\n",
    "    values_df = pd.DataFrame({\"Values\": values.numpy()})\n",
    "    values_df[\"Volatility\"] = values_df[\"Values\"].pct_change().rolling(window=5).std()  # Rolling std as volatility\n",
    "    values_df[\"SMA_5\"] = values_df[\"Values\"].rolling(window=5).mean()  # Simple moving average (5 steps)\n",
    "    values_df[\"EMA_5\"] = values_df[\"Values\"].ewm(span=5, adjust=False).mean()  # Exponential moving average\n",
    "    return values_df\n",
    "\n",
    "# Calculate metrics for real and synthetic features\n",
    "real_feature_1_metrics = calculate_metrics(real_feature_1)\n",
    "real_feature_2_metrics = calculate_metrics(real_feature_2)\n",
    "synth_feature_1_metrics = calculate_metrics(synth_feature_1)\n",
    "synth_feature_2_metrics = calculate_metrics(synth_feature_2)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Synth Feature 1\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(synth_feature_1.numpy(), label=f'Synth Feature {feature_1_idx}', color='blue')\n",
    "plt.plot(synth_feature_1_metrics[\"SMA_5\"], label='SMA (5)', linestyle='--', color='orange')\n",
    "plt.plot(synth_feature_1_metrics[\"EMA_5\"], label='EMA (5)', linestyle='--', color='green')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time step')\n",
    "plt.title(f'Synth Feature {feature_1_idx}')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Secondary y-axis for volatility\n",
    "ax_vol = plt.gca().twinx()\n",
    "ax_vol.plot(synth_feature_1_metrics[\"Volatility\"], label='Volatility', color='red', linestyle='--', linewidth=0.8)\n",
    "ax_vol.set_ylabel('Volatility')\n",
    "ax_vol.legend(loc=\"upper right\")\n",
    "\n",
    "# Real Feature 1\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(real_feature_1.numpy(), label=f'Real Feature {feature_1_idx}', color='green')\n",
    "plt.plot(real_feature_1_metrics[\"SMA_5\"], label='SMA (5)', linestyle='--', color='orange')\n",
    "plt.plot(real_feature_1_metrics[\"EMA_5\"], label='EMA (5)', linestyle='--', color='blue')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time step')\n",
    "plt.title(f'Real Feature {feature_1_idx}')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Secondary y-axis for volatility\n",
    "ax_vol = plt.gca().twinx()\n",
    "ax_vol.plot(real_feature_1_metrics[\"Volatility\"], label='Volatility', color='red', linestyle='--', linewidth=0.8)\n",
    "ax_vol.set_ylabel('Volatility')\n",
    "ax_vol.legend(loc=\"upper right\")\n",
    "\n",
    "# Synth Feature 2\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(synth_feature_2.numpy(), label=f'Synth Feature {feature_2_idx}', color='blue')\n",
    "plt.plot(synth_feature_2_metrics[\"SMA_5\"], label='SMA (5)', linestyle='--', color='orange')\n",
    "plt.plot(synth_feature_2_metrics[\"EMA_5\"], label='EMA (5)', linestyle='--', color='green')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time step')\n",
    "plt.title(f'Synth Feature {feature_2_idx}')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Secondary y-axis for volatility\n",
    "ax_vol = plt.gca().twinx()\n",
    "ax_vol.plot(synth_feature_2_metrics[\"Volatility\"], label='Volatility', color='red', linestyle='--', linewidth=0.8)\n",
    "ax_vol.set_ylabel('Volatility')\n",
    "ax_vol.legend(loc=\"upper right\")\n",
    "\n",
    "# Real Feature 2\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(real_feature_2.numpy(), label=f'Real Feature {feature_2_idx}', color='green')\n",
    "plt.plot(real_feature_2_metrics[\"SMA_5\"], label='SMA (5)', linestyle='--', color='orange')\n",
    "plt.plot(real_feature_2_metrics[\"EMA_5\"], label='EMA (5)', linestyle='--', color='blue')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time step')\n",
    "plt.title(f'Real Feature {feature_2_idx}')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Secondary y-axis for volatility\n",
    "ax_vol = plt.gca().twinx()\n",
    "ax_vol.plot(real_feature_2_metrics[\"Volatility\"], label='Volatility', color='red', linestyle='--', linewidth=0.8)\n",
    "ax_vol.set_ylabel('Volatility')\n",
    "ax_vol.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ed6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tsgm\n",
    "#import torch\n",
    "import numpy as np\n",
    "\n",
    "data = \"Hurricane\"\n",
    "file_path = f\"C:/Users/namjo/Dropbox/Time Series Tabular/Fake Data/{data}/TimeGAN_hurricane.pt\"\n",
    "TimeGAN = torch.load(file_path)\n",
    "\n",
    "file_path = f\"C:/Users/namjo/Dropbox/Time Series Tabular/Fake Data/{data}/DiffusionTS_{data}.pt\"\n",
    "Diffusion_TS = torch.load(file_path)\n",
    "\n",
    "file_path = f\"C:/Users/namjo/Dropbox/Time Series Tabular/Fake Data/{data}/Doppel_Hurricane.pt\"\n",
    "DoppelGANer_TS = torch.load(file_path)\n",
    "\n",
    "file_path = f\"C:/Users/namjo/Dropbox/Time Series Tabular/Fake Data/{data}/TSGM/tsgm_hurricane_0324.npy\"\n",
    "TSGM = torch.from_numpy(np.load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4020e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_entropy = tsgm.metrics.EntropyMetric()\n",
    "print(spec_entropy(synth_data.cpu().numpy()))\n",
    "print(spec_entropy(TimeGAN.cpu().numpy()))\n",
    "print(spec_entropy(DoppelGANer_TS.cpu().numpy()))\n",
    "print(spec_entropy(Diffusion_TS.cpu().numpy()))\n",
    "print(spec_entropy(TSGM.cpu().numpy()))\n",
    "print(spec_entropy(processed_data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de939781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsgm\n",
    "tsgm.utils.visualize_tsne_unlabeled(processed_data, synth_data, perplexity=5, markersize=75, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_metric = tsgm.metrics.MMDMetric()\n",
    "print(mmd_metric(processed_data.cpu().numpy(), synth_data.cpu().numpy()))\n",
    "print(mmd_metric(processed_data.cpu().numpy(), TimeGAN.cpu().numpy()))\n",
    "print(mmd_metric(processed_data[0:1977,:,0:7].cpu().numpy(), DoppelGANer_TS[0:1977,:,:].cpu().numpy()))\n",
    "print(mmd_metric(processed_data.cpu().numpy(), Diffusion_TS.cpu().float().numpy()))\n",
    "print(mmd_metric(processed_data.cpu().numpy(), TSGM.cpu().numpy()))\n",
    "print(mmd_metric(processed_data.cpu().numpy(), processed_data.cpu().numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
